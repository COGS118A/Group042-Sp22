{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A- Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Names\n",
    "\n",
    "- Xing Hong A15867895, xihong@ucsd.edu\n",
    "- Xiangyi Kong A16138343 xkong@ucsd.edu\n",
    "- Luning Yang A16158834 l4yang@ucsd.edu\n",
    "- Yunyi Huang A15813745 yuh022@ucsd.edu \n",
    "- Annie Fan A15932544 c7fan@ucsd.edu "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract \n",
    "\n",
    "- Proposed Problem:\n",
    "\n",
    "***Are there any clear crime type, seasonal, hourly trends in clearance status?***\n",
    "\n",
    "***How will we use data time, crime type information to predict(classify) clearance status?***\n",
    "\n",
    "- Data and Measurement: \n",
    "\n",
    "Data consists of 159K samples, 18 columns on type of crime reported, location by various attributes (lat/lon, council district, census tract) and time are included. Clearance status by Austin PD is also recorded where available. Measurement consists of timestamp, categorical, numerical data for different columns. \n",
    "\n",
    "- Data Usage: (What will we do)\n",
    "\n",
    "We will first address the correlation between crime type, date time information and clearance status by using correlation tests and independent tests. Then, we will construct different new features based on how we define the time segments for training prediction models. If the trend and correlation is clear, we can train can compare different classification models. \n",
    "\n",
    "- Performance measurement:\n",
    "\n",
    "The models’ performance will be evaluated based on our metrics. After we trained several models, we can compare them by using different criteria, for example accuracy, precision, ROC, AUC, etc..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Crime is a social problem that has been obsessing the people in the United States, and from time to time, this problem seems to become more and more severe. There are a lot of factors that would increase the crime rate, and these includes: poverty level and job availability, social level of morality, police policy, and age of the population, etc <a name=\"anote\"></a>[<sup>[1]</sup>](#sotanote) . They seem to be reasonable, because if the poverty level is high and the job availability is low, people will start to commit crimes in order to access the needs they are not able to afford through the legal way. Also, the tolerance level of immorality is high in some communities, which increases the crime eventually. Furthermore, if there is a lack in police force, there would be more space for people to commit crimes. \n",
    " \n",
    "When crimes happen, one of the most important thing is to resolve the crime, either by arrect or exceptional means, which is called clearance in professional fields [2] . Some crimes are cleared within a short time like a couple months, while some crimes are cleared after a long time which takes several years, even remain unresolved after decades. Why does it happen? Are there any features of the crimes make some of them be cleared very quickly?\n",
    "\t\n",
    "A study in Quebec, Canada focused on the similar topic suggested that the likelihood of a crime to be cleared is closely linked to the efficacy of the police departments, and it will be more likely to be cleared if the crime was commited in a wealthy community [3]. In short, the crime clearance is tied to the location of the crime. Another study focusing on the clearance of homicides, asserted that the likelihood of a crime to be cleared might be related to the offender’s demographics, drug use, and crime location, etc [4]. \n",
    " \n",
    "In this project, we would like to find out what characteristics of a crime case are likely causing the case to be cleared and build a predictor/classifier model. We will target the city of Austin, TX and perform machine learning on its data to answer the research questions we have stated in the previous section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The problems we will be focusing on are the following:\n",
    "\n",
    "***Are there any clear crime type, seasonal, hourly trends in clearance status?***\n",
    "\n",
    "***How will we use data time, crime type information to predict(classify) clearance status?***\n",
    "\n",
    "Given our definition of the problem, we can utilize the data by segmenting date time information, grouping crime types. Therefore, it is possible that we can use correlation, independence tests finding and proving the connection with clearance status (Not Cleared, Cleared by Arrest, Cleared by Exception, etc.) If the trend is clear, then we will further explore what will be the best division of time, as well as whether there is an issue of unbalanced data.\n",
    "\n",
    "Ultimately, we will build our models for classification and predict the clearance status based on our constructed features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis\n",
    "\n",
    "We hypothesize that there will be clear correlations between seasonal, hourly, crime types information and clearance status. Furthermore, it is possible for us to train a sophisticated classification model that can predict the clearance status based on given information.\n",
    "\n",
    "We decided to have this topic given the worries of the climbing crime rate. Some crimes are cleared within a short time like a couple of months, while some crimes are cleared after a long time which takes several years, and even remain unresolved after decades. We want to address the following questions: Why does it happen? Are there any features of the crimes that make some of them be cleared very quickly? With a deeper understanding of the factors that can affect crime clearance, we can improve the current efficiency by focusing on more specific targets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Link/reference to obtain it\n",
    "- https://www.kaggle.com/datasets/jboysen/austin-crime\n",
    "\n",
    "Description of the size of the dataset (# of variables, # of observations)\n",
    "- 159k - Number of Observations, 18 variables \n",
    "\n",
    "What an observation consists of\n",
    "\n",
    "- Each observation contains an address(the postal address), census_tract(the census location), clearance_date(time of clearance), clearance_status(cleared by arrest, not cleared, etc), council_district_code(Austin council district code), description(low-level description of crime), district(Austin city district), latitude(geospatial latitude coord), location(latitude, longitude), location_description(street address), longitude(geospatial longitude coordinate), primary_type(high-level description of crime), timestamp(time of report in the form of YYYY-MM-DD HH:MM:SS), unique_key(Austin record key), x_coordinate(city coordinate, not same as lat/lon), y_coordinate(city coordinate, not same as lat/lon), year(year of report, YYYY, 2014-2016), zipcode(postal zip code)\n",
    "\n",
    "What some critical variables are, how they are represented\n",
    "\n",
    "- clearance_date(time of clearance)\n",
    "- description(low-level description of crime)\n",
    "- primary_type(high-level description of crime)\n",
    "- timestamp(time of report in the form of YYYY-MM-DD HH:MM:SS)\n",
    "- clearance_status(cleared by arrest, not cleared, cleared by exception, etc)\n",
    "\n",
    "Any special handling, transformations, cleaning, etc will be needed\n",
    "\n",
    "- Handle missing data (Maybe Imputation)\n",
    "- Create a new categorical feature: seasonal_feature(Spring, Summer, Fall, Winter) from timestamp\n",
    "- Create a new time feature: hourly_feature(1am-12am) from timestamp\n",
    "- One Hot Encoding \n",
    "- Scatter Plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "- ***Are there any clear crime type, seasonal, hourly trends in clearance status?***\n",
    "\n",
    "We will be performing EDA on the given dataset and mainly using correlation matrix to see if there are any clear relationship between crime type, seasonal, hourly trends in clearance status.\n",
    "We will test the relationship with paired t-test, anova, and wilcoxon.\n",
    "This is essential as we later predict the clearance status as it could help us determine what features should be more weighted and what features are not so important in our further prediction.\n",
    "Potential use of Library: Pandas, Numpy, Scikit-Learn, Matplot\n",
    "\n",
    "- ***How will we use data time, crime type information to predict(classify) clearance status?***\n",
    "\n",
    "We will do a train/test split on our dataset and use supervised machine learning algorithms like Decision Tree Regressor to see how our model performs and set this as a baseline model.\n",
    "Then we can start using Random Forest Regression to predict one’s clearance status based on crime type, seasonal_feature(new_feature), hourly_feature(new_feature) with hyperparameter tuning to further improve our model’s performance.\n",
    "We will be using supervised machine learning algorithms like K-nearest Neighbors/Support Vector Machine to view if there are any hidden relationship between crime type, seasonal, hourly trends in clearance status to better classify clearance status.\n",
    "Model’s performance could be determined by accuracy, precision, ROC, AUC, etc.. \n",
    "Potential use of Library: Pandas, Numpy, Scikit-Learn, Matplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "In our classification tasks, we will evaluate our models by comparing the evaluation metrics such as the following:\n",
    "\n",
    "- Accuracy score (using the score method in sklearn):\n",
    "- Precision & Recall\n",
    "\n",
    "$ Accuracy = \\frac{TP+TN}{TP+TN+FP+FN} $\n",
    "\n",
    "$ Precision = \\frac{TP}{TP+FP} $ \n",
    "\n",
    "$ Recall = \\frac{TP}{TP+FN}$\n",
    "\n",
    "$ TP = True\\space Positive, TN = True\\space Negative, FP = False\\space Positive, FN = False\\space Negative $\n",
    "\n",
    "\n",
    "We are using multiple evaluation metrics because we currently do not know whether our data is balanced or not. By comparing results of different metrics, we can evaluate our models more thoroughly. We use accuracy to measure how well our model is able to put a crime record into the correct clearance status. However, accuracy might not be ideal for a dataset with disproportional clearance status.We will use alternative metrics such as precision and recall. These two metrics are good at detecting whether a model can predict a category with small proportion in the dataset correctly. For example, if the dataset only contains a small proportion of a certain clearance status but we really care about whether we can predict that status correctly. We should try these two metrics. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data collection\n",
    "\n",
    "   + We need to ensure that the data collectors get informed consent from the participants while collecting the data. Although they may not obtain the consent from each individual criminal, the data may still be collected and analyzed for public interest. We don’t know the exact procedures for the data collection, so we make no comment here.\n",
    "\n",
    "   + The biases of our data collection is not an issue to consider, since the data is collected by Austin PD and published by Kaggle. We believe our data is credible.\n",
    "\n",
    "   + The dataset only contains information about the address, time, and crime type of each crime. There’s no Personal Identifiable Information (PII) such as age, gender, or height of each criminal. So we should not worry about exposing a person’s privacy.\n",
    "\n",
    "- Data storage\n",
    "\n",
    "   + This should not be an issue to consider, since we don’t store our data in our own local devices. Every time we run our project, we just need to pull the data from Kaggle. We should always be aware of the update of the data.\n",
    "\n",
    "- Analysis\n",
    "\n",
    "   + We need to ensure that our results are fully supported by our data. We should avoid manipulating (p-hacking) our data to make it in favor of our hypotheses. If there’s no special crime type, seasonal, hourly trends in clearance status, we won’t modify our data to create a trend. Moreover, if we fail to use information such as time or crime type to predict the clearance status, we should report it honestly. We should not exaggerate the correlation between the independent variables and the clearance status. \n",
    "\n",
    "   + We need to address the following questions before testing our hypothesis:\n",
    "      * Are the records comprehensive, accurate and complete?\n",
    "      * If there exists some incomplete or inaccurate records, how are we going to filter them out?\n",
    "      * Does the government withhold some criminal records?\n",
    "      * Does this dataset serve some political purposes such as degrading certain districts in Austin city?\n",
    "\n",
    "- Modeling\n",
    "\n",
    "   + Proxy discrimination: Since our dataset doesn’t contain any discriminatory variable such as race or gender, this should not be an issue for us to consider. \n",
    "\n",
    "   + Fairness across groups: We need to make sure our model result is fair across all districts in Austin city.\n",
    "\n",
    "   + Explainability: We should explain our modeling procedures clearly and straightforwardly. \n",
    "\n",
    "   + Metric selection: We should use appropriate metrics to measure our model’s performance and be aware of its shortcomings. In our case, we will possibly use accuracy, precision, or recall to assess our clearance status classification performance.\n",
    "\n",
    "   + Communicate bias: We should discuss the shortcomings and future improvements of our modeling. \n",
    "\n",
    "- Deployment\n",
    "\n",
    "   + Redress: redress should not be a concern, since no PII are released in the dataset. We should not worry that participants could get hurt.\n",
    "\n",
    "   + Concept drift: We should always update our model with the most recent data. We understand that the definitions of terms such as crime types or clearance can be changed over time. So we should adjust our model accordingly. \n",
    "\n",
    "   + Roll back: We need to make sure that our results will not discriminate against people in any particular district. If that happens, we should withdraw our research and revise it. \n",
    "\n",
    "   + Unintended use: we should clearly list our research purpose and limitations to prevent other entities from misusing our research result.\n",
    "\n",
    "- Reproducibility and replicability\n",
    "\n",
    "   + We will publish our research to a public github repository to make our research reproducible. Our people can reproduce our research by following our procedure. They might get a different result due to the update of the criminal records.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Be on time to meetings\n",
    "- Be polite and respectful to team members during discussions\n",
    "- Finish tasks on time\n",
    "- Do not procrastinate\n",
    "- Ask for help/offer help when needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 4/23  |  8 PM |  Brainstorm topics/questions (all); Read the proposal requirements  | Determine what topics we are interested in; Finding the dataset; Draft the outline of research proposal; Set the division of works | \n",
    "| 4/24  |  7 PM |  Each team member finishes their work division. Edit, finalize, and submit proposal | Proofread other members’ work division. Give other members suggestions for improvements. | \n",
    "| 5/1  | 6 PM  | Import & Wrangle Data, do some EDA  | Review/Edit wrangling/EDA; Discuss Analysis Plan   |\n",
    "| 5/8  | 7 PM  | Finalize and submit wrangling/EDA checkpoint; Begin programming for project | Discuss/edit project code; Complete project   |\n",
    "| 5/13  | Before 12PM  | Check Point | Finish Check Point problems |\n",
    "| 5/22  | 12 PM  | Complete analysis; Draft results/conclusion/discussion | Discuss/edit full project |\n",
    "| 5/29  | 12 PM  | Office hour and review | Keep up progress  |\n",
    "| 6/5  | Before 11:59 PM  | Finalizing report, check what's missing | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"anote\"></a>1.[^](#anote):  Thompson, J. (9 Dec 2018) Factors influencing the crime rate. Legal Beagle. https://legalbeagle.com/5969328-factors-influencing-crime-rate.html <br> \n",
    "<a name=\"bnote\"></a>2.[^](#bnote):  Offenses Cleared. FBI. https://ucr.fbi.gov/crime-in-the-u.s/2010/crime-in-the-u.s.-2010/clearances#:~:text=Cleared%20by%20exceptional%20means,In%20certain%20situations&text=The%20agency%20must%20have%3A,be%20taken%20into%20custody%20immediately. <br> \n",
    "<a name=\"cnote\"></a>3.[^](#cnote):  Paré, P. P., Felson, R. B., & Ouimet, M. (2007). Community variation in crime clearance: A multilevel analysis with comments on assessing police performance. Journal of quantitative criminology, 23(3), 243-258. https://link.springer.com/article/10.1007/s10940-007-9028-0 <br> \n",
    "<a name=\"dnote\"></a>4.[^](#dnote):  Wellford, C., & Cronin, J. (1999). An analysis of variables affecting the clearance of homicides: A multistate study. https://ncvc.dspacedirect.org/handle/20.500.11990/4057 <br> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
